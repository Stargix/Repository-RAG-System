{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Github Repo RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import typing\n",
    "import dotenv\n",
    "import json\n",
    "import os\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement a RAG system over a code repository, we first need to access its files. There are two alternatives for this:\n",
    "\n",
    "**Clone the Repository Locally**: The code below generates a JSON with the content of the files by cloning the repository locally. If you don't want to clone the repository, omit this chunk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# URL of the repository to clone\n",
    "repo_url = \"https://github.com/viarotel-org/escrcpy\"\n",
    "repo_dir = \"repo_temp\"\n",
    "\n",
    "# Clone the repository if it doesn't exist\n",
    "if not os.path.exists(repo_dir):\n",
    "    subprocess.run([\"git\", \"clone\", repo_url, repo_dir])\n",
    "\n",
    "# Get the files in the repository\n",
    "def get_repo_files(directory):\n",
    "    files_dict = {}\n",
    "    for file_path in Path(directory).rglob(\"*\"):\n",
    "        if file_path.is_file():\n",
    "            try:\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\", errors='ignore') as f:\n",
    "                    content = f.read()\n",
    "                relative_path = file_path.relative_to(directory)\n",
    "                files_dict[str(relative_path)] = content\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file_path}: {e}\")\n",
    "    return files_dict\n",
    "\n",
    "files_dict = get_repo_files(repo_dir)\n",
    "\n",
    "# Save the files in a JSON file\n",
    "with open('./data/files.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(files_dict, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Total files obtained: {len(files_dict)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Use the GitHub API**: The second alternative is to use the GitHub API to get access to the files, but you need to be logged in to GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from repo_api import get_github_files\n",
    "\n",
    "url = \"https://github.com/viarotel-org/escrcpy\"\n",
    "token = os.getenv(\"GITHUB_TOKEN\")\n",
    "\n",
    "files_dict = get_github_files(url, token)\n",
    "\n",
    "# Save de dict as a JSON (to not exhaust the API calls)\n",
    "with open('./data/files.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(files_dict, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Show results\n",
    "print(f\"Total obtained files: {len(files_dict)}\")\n",
    "for path, content in list(files_dict.items())[:3]:  # Show first 3 files head as example\n",
    "    print(f\"\\nFile: {path}\")\n",
    "    print(f\"Content (first 100 characters): {content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide into chunks\n",
    "These files are mostly code resources, which can contain a lot of information irrelevant to a specific query and cause noise. To reduce this noise and improve the similarity of the embeddings, we first divide the files into chunks. The size of these chunks determines the balance between context and relevance. Very small chunks may not provide enough context, while very large chunks may include too much irrelevant information. The chosen size is 2500 characters per chunk to create aproximately 7 chunks per file.\n",
    "\n",
    "It is also important to not cut the chunks in bad ways. For this reason, we use RecursiveCharacterTextSplitter from langchain, which preserves larger units, which will benefit the structure of the code as functions or classes as well as other files structured in paragraphs such as markdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter \n",
    "from langchain.schema import Document\n",
    "\n",
    "def get_chunks(doc, chunk_size=2500, chunk_overlap=50) -> typing.List[dict]:\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, # Size of each chunk in characters\n",
    "        chunk_overlap=chunk_overlap, # Overlap between consecutive chunks\n",
    "        length_function=len, # Function to compute the length of the text\n",
    "        add_start_index=True, # Flag to add start index to each chunk\n",
    "    )\n",
    "    # Split document into smaller chunks using text splitter\n",
    "    chunks = text_splitter.split_documents(doc)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "if 'files_dict' not in globals():\n",
    "    with open('./data/files.json', 'r', encoding='utf-8') as f:\n",
    "        files_dict = json.load(f)\n",
    "\n",
    "chunked_files = [\n",
    "    Document(page_content=file_content, metadata={\"source\": file_path})\n",
    "    for file_path, file_content in files_dict.items()\n",
    "]\n",
    "chunks = get_chunks(chunked_files, chunk_size=2500, chunk_overlap=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Indexing\n",
    "\n",
    "Before we can perform any searches, we need to index the repository files. This involves loading the encoder model and generating embeddings for each chunk of text.\n",
    "\n",
    "### Load the encoder_model\n",
    "\n",
    "The encoder model is loaded using the `SentenceTransformer` from the `sentence_transformers` library. The model is loaded onto the GPU if available, otherwise, it defaults to the CPU. The model is configured to use float16 precision to reduce memory usage and speed up inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "except ImportError:\n",
    "    device = \"cpu\"\n",
    "# Load the model with float16 support to reduce memory usage and speed up inference\n",
    "encoder_model = SentenceTransformer(\"ibm-granite/granite-embedding-107m-multilingual\", device=device, model_kwargs={\"torch_dtype\": \"float16\"})\n",
    "# Verifies the use of the GPU\n",
    "print(f\"Using device: {encoder_model.device}\")\n",
    "\n",
    "\n",
    "#\"ibm-granite/granite-embedding-107m-multilingual\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize text\n",
    "Once we have the chunks, we need to normalize the text before encoding it, to ensure that the embeddings are consistent. We will use the langchain library to normalize the text, where we preserve the original indetation while removing extra spaces and newlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def normalize_code(code: str) -> str:\n",
    "    code = code.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
    "    # Preserve indentation while normalizing the rest of the line\n",
    "    lines = []\n",
    "    for line in code.split('\\n'):\n",
    "        # Count initial spaces\n",
    "        leading_space_count = len(line) - len(line.lstrip(' \\t'))\n",
    "        leading_space = line[:leading_space_count]   \n",
    "        # Normalize the rest of the line\n",
    "        rest_of_line = re.sub(r'[ \\t]+', ' ', line[leading_space_count:])\n",
    "        lines.append(leading_space + rest_of_line)\n",
    "        \n",
    "    code = '\\n'.join(lines)\n",
    "    # Eliminate consecutive blank lines\n",
    "    code = re.sub(r'\\n{3,}', '\\n\\n', code)\n",
    "    \n",
    "    return code.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save embeddings\n",
    "We create an embedding for each chunk using the `SentenceTransformer` model. The embeddings are stored in a ChromaDB collection for efficient similarity search. The process involves normalizing the code, generating embeddings in batches, and adding them to the ChromaDB collection.\n",
    "\n",
    "ChromaDB is used because it provides a highly efficient and scalable solution for similarity search, allowing us to quickly retrieve the most relevant chunks of text based on their embeddings. This is crucial for handling large code repositories and ensuring fast and accurate search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Chunks: 100%|██████████| 653/653 [07:34<00:00,  1.44it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import chromadb\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "# Delete the existing ChromaDB collection if it exists\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "\n",
    "if chroma_client.get_collection(\"escrcpy_repo_embeddings\"):\n",
    "    chroma_client.delete_collection(\"escrcpy_repo_embeddings\")\n",
    "\n",
    "chroma_collection = chroma_client.get_or_create_collection(\n",
    "    name=\"escrcpy_repo_embeddings\",\n",
    "    metadata={\"hnsw:space\": \"cosine\"},\n",
    ")\n",
    "\n",
    "# Process in batches\n",
    "for i in tqdm(range(0, len(chunks), batch_size), desc=\"Processing Chunks\"):\n",
    "    batch = chunks[i:i+batch_size]\n",
    "    batch_texts = [normalize_code(chunk.page_content) for chunk in batch]\n",
    "    batch_embeddings = encoder_model.encode(batch_texts)\n",
    "    \n",
    "    # Prepare batch data for ChromaDB\n",
    "    batch_ids = [f\"chunk_{i + j}\" for j in range(len(batch))]\n",
    "    batch_documents = [chunk.page_content for chunk in batch]\n",
    "    batch_metadatas = [{\n",
    "        \"source\": chunk.metadata[\"source\"],\n",
    "        \"start_index\": chunk.metadata.get(\"start_index\", 0)\n",
    "    } for chunk in batch]\n",
    "    \n",
    "    # Add entire batch at once\n",
    "    chroma_collection.add(\n",
    "        embeddings=[emb.tolist() for emb in batch_embeddings],\n",
    "        documents=batch_documents,\n",
    "        metadatas=batch_metadatas,\n",
    "        ids=batch_ids\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve\n",
    "Once we have created the Chroma database, we need to create a function to retrieve similar encoded files using a natural language query. This function will allow us to search the repository for relevant files based on the embeddings generated from the text chunks. To avoid repeating the same file, the code concatenates chunks from the same source and ensures receiving the desired results from different files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most related file\n",
      "electron\\resources\\extra\\linux\\android-platform-tools\\adb\n"
     ]
    }
   ],
   "source": [
    "def search_repository(question, n_results=5, chroma_collection=None):  \n",
    "    if chroma_collection is None:\n",
    "        chroma_collection = chroma_client.get_collection(\"escrcpy_repo_embeddings\")\n",
    "    \n",
    "    # Normalize the question text and generate embedding\n",
    "    normalized_question = normalize_code(question)\n",
    "    question_embedding = encoder_model.encode(normalized_question)\n",
    "    \n",
    "    combined_results = {}\n",
    "    seen_sources = set()\n",
    "    max_attempts = n_results * 3  # Avoid infinite loop\n",
    "    offset = 0\n",
    "    \n",
    "    # Keep querying until we have n_results unique sources or reach max_attempts\n",
    "    while len(combined_results) < n_results and offset < max_attempts:\n",
    "        # Calculate how many more results we need\n",
    "        remaining = n_results - len(combined_results)\n",
    "        \n",
    "        results = chroma_collection.query(\n",
    "            query_embeddings=[question_embedding.tolist()],\n",
    "            n_results=remaining + offset,  # Get extra results to account for duplicates\n",
    "            include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "        )\n",
    "        \n",
    "        for i in range(len(results[\"documents\"][0])):\n",
    "            source = results[\"metadatas\"][0][i][\"source\"]\n",
    "            content = results[\"documents\"][0][i]\n",
    "            similarity = 1 - results[\"distances\"][0][i]\n",
    "            \n",
    "            # Skip if we've already seen this source\n",
    "            if source in seen_sources:\n",
    "                combined_results[source][\"content\"] += \"\\n\\n\" + content\n",
    "                combined_results[source][\"similarity\"] = max(combined_results[source][\"similarity\"], similarity)\n",
    "            \n",
    "            else:\n",
    "                seen_sources.add(source)\n",
    "                combined_results[source] = {\n",
    "                    \"content\": content,\n",
    "                    \"source\": source,\n",
    "                    \"similarity\": similarity\n",
    "                }   \n",
    "        # Increase offset to get new results in the next query\n",
    "        offset += remaining\n",
    "    \n",
    "    # Convert dictionary back to list and sort by similarity\n",
    "    formatted_results = list(combined_results.values())\n",
    "    formatted_results.sort(key=lambda x: x[\"similarity\"], reverse=True)\n",
    "    \n",
    "    return formatted_results[:n_results]\n",
    "\n",
    "\n",
    "q = \"How does the repository handle IPv6 addresses in ADB commands?\"\n",
    "results = search_repository(q, n_results=5)\n",
    "\n",
    "print(f\"Most related file\")\n",
    "print(results[0][\"source\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have finished the indexing part, we import the test JSON to evaluate the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/escrcpy-commits-generated.json\", \"r\") as f:\n",
    "        test_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "The test JSON contains a list of questions and their corresponding files. We will use this data to assess the accuracy and recall of our search system. The evaluation process involves running each question through the query function and comparing the predicted sources with the actual files. First we create a dataframe from and add the predicted files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>files</th>\n",
       "      <th>question</th>\n",
       "      <th>predicted_sources</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[src/components/PreferenceForm/components/Sele...</td>\n",
       "      <td>How does the SelectDisplay component handle th...</td>\n",
       "      <td>[src/components/PreferenceForm/components/Sele...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[electron/exposes/adb/helpers/index.js, electr...</td>\n",
       "      <td>How does the repository handle IPv6 addresses ...</td>\n",
       "      <td>[electron/resources/extra/linux/android-platfo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[electron/helpers/edger/index.js]</td>\n",
       "      <td>How does the edge hiding and snapping mechanis...</td>\n",
       "      <td>[electron/main.js, electron/helpers/edger/inde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[README-CN.md, README-RU.md, README.md]</td>\n",
       "      <td>Unable to detect device</td>\n",
       "      <td>[electron/resources/extra/win/android-platform...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[src/pages/device/components/MirrorAction/inde...</td>\n",
       "      <td>What functionality does the component provide ...</td>\n",
       "      <td>[src/locales/languages/zh-CN.json, electron/re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               files  \\\n",
       "0  [src/components/PreferenceForm/components/Sele...   \n",
       "1  [electron/exposes/adb/helpers/index.js, electr...   \n",
       "2                  [electron/helpers/edger/index.js]   \n",
       "3            [README-CN.md, README-RU.md, README.md]   \n",
       "4  [src/pages/device/components/MirrorAction/inde...   \n",
       "\n",
       "                                            question  \\\n",
       "0  How does the SelectDisplay component handle th...   \n",
       "1  How does the repository handle IPv6 addresses ...   \n",
       "2  How does the edge hiding and snapping mechanis...   \n",
       "3                            Unable to detect device   \n",
       "4  What functionality does the component provide ...   \n",
       "\n",
       "                                   predicted_sources  \n",
       "0  [src/components/PreferenceForm/components/Sele...  \n",
       "1  [electron/resources/extra/linux/android-platfo...  \n",
       "2  [electron/main.js, electron/helpers/edger/inde...  \n",
       "3  [electron/resources/extra/win/android-platform...  \n",
       "4  [src/locales/languages/zh-CN.json, electron/re...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_test = pd.DataFrame(test_data)\n",
    "df_evaluation = df_test.copy()\n",
    "questions = df_test[\"question\"].tolist()\n",
    "\n",
    "sources = []\n",
    "for query in questions:\n",
    "    metadata = search_repository(query, n_results=10)\n",
    "    source = [result[\"source\"] for result in metadata]\n",
    "    sources.append(source)\n",
    "\n",
    "    sources = [[source.replace(\"\\\\\", \"/\") for source in source_list] for source_list in sources]\n",
    "\n",
    "df_evaluation[\"predicted_sources\"] = sources\n",
    "\n",
    "display(df_evaluation.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the predictions, we need to calculate the recall@10 metric. This metric measures how many relevant files were matched from all possible relevant files, considering only the first 10 retrieved files. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Recall@10: 0.49\n"
     ]
    }
   ],
   "source": [
    "def recall_at_10(relevant, retrieved):\n",
    "    top_10 = retrieved[:10]\n",
    "    relevant_set = set(relevant)\n",
    "    retrieved_set = set(top_10)\n",
    "\n",
    "    # Calculate hits and total relevant items\n",
    "    hits = len(relevant_set & retrieved_set)\n",
    "    total_relevant = len(relevant_set)\n",
    "    \n",
    "    return hits / total_relevant\n",
    "\n",
    "df_recall = df_evaluation.copy()\n",
    "df_recall[\"recall@10\"] = df_recall.apply(\n",
    "    lambda row: recall_at_10(row[\"files\"], row[\"predicted_sources\"]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "average_recall = df_recall[\"recall@10\"].mean()\n",
    "print(f\"Average Recall@10: {average_recall:.2f}\")\n",
    "\n",
    "df_recall.to_csv(\"./data/evaluation_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After trying many embedding models, chunk sizes, and different distances, the maximum recall achieved has been 0.45. Attempts to add query expansion did not improve the metric. Analyzing the synthetic dataset, we can see that there are many general queries as well as many files related to others, which can lead to lower recall scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM-Generated Answer Summaries for Retrieved Code\n",
    "\n",
    "In this section, we integrate a Language Model (LLM) to generate concise and relevant summaries for the retrieved code snippets. This helps in understanding the context and functionality of the code without manually inspecting each file.\n",
    "\n",
    "#### Steps to Generate Summaries:\n",
    "\n",
    "1. **Search Repository**: We first search the repository using a natural language query to retrieve the most relevant code snippets. This is done using the `search_repository` function which leverages the embeddings stored in ChromaDB.\n",
    "\n",
    "2. **Generate Context**: The retrieved code snippets are concatenated to form a context that provides a comprehensive view of the relevant code.\n",
    "\n",
    "3. **Formulate Prompt**: A prompt is created for the LLM, instructing it to generate a concise answer to the query using the provided context.\n",
    "\n",
    "4. **Generate Answer**: The LLM processes the prompt and generates a summary that directly answers the query, utilizing the context from the retrieved code snippets.\n",
    "\n",
    "#### Example implementation with gemini-2.0-flash:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sponsor dialog displays sponsorship images dynamically using the `imageList` array in the component's data. This array holds objects with `src` (image path) and `alt` (alt text) properties.  The `v-for` directive iterates through this array, creating an `el-image` component for each item.  The `src` and `alt` properties of each image are bound to the corresponding properties in the `imageList` item. The image sources are directly imported into the component.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "api_key = os.getenv(\"LLM_API_KEY\")\n",
    "\n",
    "def generate_answer(question):\n",
    "    results = search_repository(question, n_results=5)\n",
    "    context = \"\\n\\n\".join([result[\"content\"] for result in results])\n",
    "\n",
    "    prompt = f\"Answer this question: {question} directly. If this context files are helpful you can use them to reinforce your answer, answer concisely:\\n\\n{context}\\n\\n\"\n",
    "    \n",
    "    client = genai.Client(api_key=api_key)\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        contents=prompt)\n",
    "\n",
    "    return response.text\n",
    "\n",
    "print(generate_answer(\"How is the sponsor dialog implemented to display sponsorship images dynamically?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
